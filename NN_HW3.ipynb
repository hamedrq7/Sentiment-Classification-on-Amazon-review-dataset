{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uVcFK_kk_jg",
        "outputId": "8cfed09e-8e23-40ad-c2ed-96c725ac1be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/nn hw3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esyh9uQwlTW_",
        "outputId": "fd940f23-a888-4ae8-d71c-d1ae98161d23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/16pPvnYNuR5IKHD3x4aWZ2qNj3oNEGnCa/nn hw3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import pickle"
      ],
      "metadata": {
        "id": "WZIo1Ubqlbik"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/Magazine_Subscriptions.json.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1qCakP7leQC",
        "outputId": "91a5d551-7ed0-4a45-b60f-1b91c8122a81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-13 17:56:04--  https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/Magazine_Subscriptions.json.gz\n",
            "Resolving datarepo.eng.ucsd.edu (datarepo.eng.ucsd.edu)... 132.239.8.30\n",
            "Connecting to datarepo.eng.ucsd.edu (datarepo.eng.ucsd.edu)|132.239.8.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12875273 (12M) [application/x-gzip]\n",
            "Saving to: ‘Magazine_Subscriptions.json.gz.2’\n",
            "\n",
            "Magazine_Subscripti 100%[===================>]  12.28M  7.26MB/s    in 1.7s    \n",
            "\n",
            "2023-06-13 17:56:07 (7.26 MB/s) - ‘Magazine_Subscriptions.json.gz.2’ saved [12875273/12875273]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "where\n",
        "\n",
        "- `reviewerID` - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
        "- `asin` - ID of the product, e.g. 0000013714\n",
        "- `reviewerName` - name of the reviewer\n",
        "- `vote` - helpful votes of the review\n",
        "- `style` - a disctionary of the product metadata, e.g., \"Format\" is \"Hardcover\"\n",
        "- `reviewText` - text of the review\n",
        "- `overall` - rating of the product\n",
        "- `summary` - summary of the review\n",
        "- `unixReviewTime` - time of the review (unix time)\n",
        "- `reviewTime` - time of the review (raw)\n",
        "- `image` - images that users post after they have received the product"
      ],
      "metadata": {
        "id": "tV4tWq09liHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  indx = 0\n",
        "  for l in g:\n",
        "    yield json.loads(l)\n",
        "\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "df = getDF('Magazine_Subscriptions.json.gz')\n",
        "df = df[['overall', 'reviewText']]\n",
        "df['overall'] = df['overall'].astype(np.int32) - 1\n",
        "df.dropna(inplace=True)\n",
        "df = df[list(df.columns)[::-1]]\n",
        "df.to_csv('amazon.csv', index = False)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2obCDSxQligh",
        "outputId": "276fbe32-abb7-4e00-e173-594a34a339f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              reviewText  overall\n",
              "0      for computer enthusiast, MaxPC is a welcome si...        4\n",
              "1      Thank god this is not a Ziff Davis publication...        4\n",
              "2      Antiques Magazine is a publication made for an...        2\n",
              "3      This beautiful magazine is in itself a work of...        4\n",
              "4                              A great read every issue.        4\n",
              "...                                                  ...      ...\n",
              "89684  This was a nice surprise for my boyfriend. He ...        4\n",
              "89685  Magazine looks like it is printed on recycled ...        0\n",
              "89686  cant go wrong with an SI subscription\\nvery pl...        4\n",
              "89687  This magazine is by far my all time favorite o...        4\n",
              "89688             Nice magazine. Good info and articles.        4\n",
              "\n",
              "[89656 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-688a633f-e741-4fcc-802f-815807a87e48\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>overall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>for computer enthusiast, MaxPC is a welcome si...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Thank god this is not a Ziff Davis publication...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Antiques Magazine is a publication made for an...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This beautiful magazine is in itself a work of...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A great read every issue.</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89684</th>\n",
              "      <td>This was a nice surprise for my boyfriend. He ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89685</th>\n",
              "      <td>Magazine looks like it is printed on recycled ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89686</th>\n",
              "      <td>cant go wrong with an SI subscription\\nvery pl...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89687</th>\n",
              "      <td>This magazine is by far my all time favorite o...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89688</th>\n",
              "      <td>Nice magazine. Good info and articles.</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89656 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-688a633f-e741-4fcc-802f-815807a87e48')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-688a633f-e741-4fcc-802f-815807a87e48 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-688a633f-e741-4fcc-802f-815807a87e48');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del df"
      ],
      "metadata": {
        "id": "APeZYTAOlrAu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torchdata.datapipes.iter import IterableWrapper, FileOpener\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Sampler, Dataset\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "lNU7KIKLl22G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBatchSamplerSimilarLength(Sampler):\n",
        "    def __init__(self, dataset, batch_size, tokenizer, indices=None, shuffle=True):\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        # get the indices and length\n",
        "        self.indices = [(i, len(tokenizer(s[0]))) for i, s in enumerate(dataset)]\n",
        "        # if indices are passed, then use only the ones passed (for ddp)\n",
        "        if indices is not None:\n",
        "            self.indices = torch.tensor(self.indices)[indices].tolist()\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.indices)\n",
        "\n",
        "        pooled_indices = []\n",
        "        # create pool of indices with similar lengths\n",
        "        for i in range(0, len(self.indices), self.batch_size * 100):\n",
        "          pooled_indices.extend(sorted(self.indices[i:i + self.batch_size * 100], key=lambda x: x[1]))\n",
        "        self.pooled_indices = [x[0] for x in pooled_indices]\n",
        "\n",
        "        # Comment in for validation\n",
        "        #self.pooled_lengths = [x[1] for x in pooled_indices]\n",
        "        #print(self.pooled_lengths)\n",
        "        #print(self.pooled_indices)\n",
        "\n",
        "        # yield indices for current batch\n",
        "        batches = [self.pooled_indices[i:i + self.batch_size] for i in\n",
        "                   range(0, len(self.pooled_indices), self.batch_size)]\n",
        "\n",
        "        if self.shuffle:\n",
        "            random.shuffle(batches)\n",
        "        for batch in batches:\n",
        "            yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pooled_indices) // self.batch_size"
      ],
      "metadata": {
        "id": "5QaaZclxmRvv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir(directory: str):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def custom_plot_training_stats(acc_hist, loss_hist, phase_list, title: str, dir: str):\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=[14, 6], dpi=100)\n",
        "\n",
        "    for phase in phase_list:\n",
        "        lowest_loss_x = np.argmin(np.array(loss_hist[phase]))\n",
        "        lowest_loss_y = loss_hist[phase][lowest_loss_x]\n",
        "\n",
        "        ax1.annotate(\"{:.4f}\".format(lowest_loss_y), [lowest_loss_x, lowest_loss_y])\n",
        "        ax1.plot(loss_hist[phase], '-x', label=f'{phase} loss', markevery = [lowest_loss_x])\n",
        "\n",
        "        ax1.set_xlabel(xlabel='epochs')\n",
        "        ax1.set_ylabel(ylabel='loss')\n",
        "\n",
        "        ax1.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
        "        ax1.legend()\n",
        "        ax1.label_outer()\n",
        "\n",
        "    # acc:\n",
        "    for phase in phase_list:\n",
        "        highest_acc_x = np.argmax(np.array(acc_hist[phase]))\n",
        "        highest_acc_y = acc_hist[phase][highest_acc_x]\n",
        "\n",
        "        ax2.annotate(\"{:.4f}\".format(highest_acc_y), [highest_acc_x, highest_acc_y])\n",
        "        ax2.plot(acc_hist[phase], '-x', label=f'{phase} loss', markevery = [highest_acc_x])\n",
        "\n",
        "        ax2.set_xlabel(xlabel='epochs')\n",
        "        ax2.set_ylabel(ylabel='acc')\n",
        "\n",
        "        ax2.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
        "        ax2.legend()\n",
        "        #ax2.label_outer()\n",
        "\n",
        "    fig.suptitle(f'{title}')\n",
        "\n",
        "    mkdir(dir)\n",
        "    plt.savefig(f'{dir}/{title}_ acc loss.jpg')\n",
        "    plt.clf()"
      ],
      "metadata": {
        "id": "cLEs6joV7evq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        self.rnn = torch.nn.LSTM(input_size=embedding_dim,\n",
        "                                 hidden_size=hidden_dim,\n",
        "                                 num_layers=num_layers\n",
        "                                 )\n",
        "\n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text dim: [sentence length, batch size]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded dim: [sentence length, batch size, embedding dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "        # output dim: [sentence length, batch size, hidden dim]\n",
        "        # hidden dim: [num_layers, batch size, hidden dim]\n",
        "\n",
        "        ### Changed it\n",
        "        ### hidden.squeeze_(0)\n",
        "        # hidden_final = hidden[-1]\n",
        "        # hidden_final: [batch size, hidden dim]\n",
        "        # output = self.fc(hidden_final)\n",
        "\n",
        "        clf_output = output[-1] # [batch size, hidden dim]\n",
        "        output = self.fc(clf_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class RNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        self.rnn = torch.nn.RNN(input_size=embedding_dim,\n",
        "                                 hidden_size=hidden_dim,\n",
        "                                 num_layers=num_layers,\n",
        "                                 nonlinearity='relu'\n",
        "                                 )\n",
        "\n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text dim: [sentence length, batch size]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded dim: [sentence length, batch size, embedding dim]\n",
        "\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        # output dim: [sentence length, batch size, hidden dim]\n",
        "        # hidden dim: [num_layers, batch size, hidden dim]\n",
        "\n",
        "        clf_output = output[-1] # [batch size, hidden dim]\n",
        "        output = self.fc(clf_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class GRU(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        # tru dropout\n",
        "        self.gru = torch.nn.GRU(input_size=embedding_dim,\n",
        "                                 hidden_size=hidden_dim,\n",
        "                                 num_layers=num_layers,\n",
        "                                 )\n",
        "\n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text dim: [sentence length, batch size]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded dim: [sentence length, batch size, embedding dim]\n",
        "\n",
        "        output, hidden = self.gru(embedded)\n",
        "        # output dim: [sentence length, batch size, hidden dim]\n",
        "        # hidden dim: [num_layers, batch size, hidden dim]\n",
        "\n",
        "        clf_output = output[-1] # [batch size, hidden dim]\n",
        "        output = self.fc(clf_output)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "7zrCmEjUmGje"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class sentiment_clf():\n",
        "    def __init__(self, device, rand_seed = 123):\n",
        "        self.RANDOM_SEED = rand_seed\n",
        "        torch.manual_seed(self.RANDOM_SEED)\n",
        "        self.device = device\n",
        "\n",
        "    def load_data(self, csv_name: str, debug=False):\n",
        "        self.csv_name = csv_name\n",
        "        self.datapipe = IterableWrapper([f\"{csv_name}\"])\n",
        "        self.datapipe = FileOpener(self.datapipe, mode='b')\n",
        "        self.datapipe = self.datapipe.parse_csv(skip_lines=1) # skip_lines=1 skip headers\n",
        "\n",
        "        if debug:\n",
        "            indx = 0\n",
        "            for sample in self.datapipe:\n",
        "                print(sample)\n",
        "                indx += 1\n",
        "                if indx > 6:\n",
        "                    break\n",
        "\n",
        "        N_ROWS = len(list(self.datapipe))\n",
        "        self.train_dp, self.valid_dp, self.test_dp = self.datapipe.random_split(total_length=N_ROWS, weights={\"train\": 0.8, \"valid\": 0.1, \"test\": 0.1}, seed=self.RANDOM_SEED)\n",
        "        if debug:\n",
        "            print('N_ROWS:', N_ROWS)\n",
        "            print(f'Num Train: {len(list(self.train_dp))}')\n",
        "            print(f'Num Valid: {len(list(self.valid_dp))}')\n",
        "            print(f'Num Test: {len(list(self.test_dp))}')\n",
        "\n",
        "\n",
        "    def build_vocab(self, vocab_size, tokenizer_name: str = 'spacy', save_vocab=True, debug=False):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "\n",
        "        if tokenizer_name == 'spacy':\n",
        "            lang = 'en_core_web_sm'\n",
        "        else:\n",
        "            lang = 'en'\n",
        "        self.tokenizer = get_tokenizer(tokenizer=tokenizer_name, language=lang)\n",
        "\n",
        "        def yield_tokens(data_iter):\n",
        "            for text, _ in data_iter:\n",
        "                yield self.tokenizer(text)\n",
        "\n",
        "        self.vocab = build_vocab_from_iterator(yield_tokens(self.train_dp), specials=[\"<UNK>\", \"<PAD>\"], max_tokens=self.vocab_size)\n",
        "        self.vocab.set_default_index(self.vocab[\"<UNK>\"])\n",
        "        if debug:\n",
        "            print(\"Vocabulary size: \", len(self.vocab))\n",
        "            print(self.vocab.get_itos()[:10]) # itos = integer-to-string\n",
        "            print(f\"Default index: {self.vocab.get_default_index()}\")\n",
        "\n",
        "        self.padding_value=self.vocab['<PAD>']\n",
        "\n",
        "        self.text_transform = lambda x: [self.vocab[token] for token in self.tokenizer(x)]\n",
        "        self.label_transform = lambda x: int(x)\n",
        "\n",
        "        if debug:\n",
        "            print(f\"the: {self.vocab['the']}\")\n",
        "            # And an indirect way, using get_stoi() to get a dictionary of tokens and values\n",
        "            print(f\"the: {self.vocab.get_stoi()['the']}\") # stoi = string-to-integer\n",
        "            # What is the padding value?\n",
        "            print(f\"<PAD>: {self.vocab['<PAD>']}\")\n",
        "\n",
        "            # Print out the output of text_transform\n",
        "            print(\"input to the text_transform:\", \"here is an example\")\n",
        "            print(\"output of the text_transform:\", self.text_transform(\"here is an example\"))\n",
        "            for text, label in self.train_dp:\n",
        "                print(label)\n",
        "                print(text)\n",
        "                first_text_tokens = self.text_transform(text)\n",
        "                print(self.text_transform(text))\n",
        "                print(self.label_transform(label))\n",
        "                break\n",
        "\n",
        "            vocab_itos = self.vocab.get_itos()\n",
        "            vec_vocab_itos = np.vectorize(lambda x: vocab_itos[x])\n",
        "            print(vec_vocab_itos(first_text_tokens))\n",
        "\n",
        "    def create_dataloaders(self, batch_size, debug=False):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        def collate_batch(batch):\n",
        "            text_list, label_list = [], []\n",
        "            for (_text, _label) in batch:\n",
        "                processed_text = torch.tensor(self.text_transform(_text))\n",
        "                text_list.append(processed_text)\n",
        "                label_list.append(self.label_transform(_label))\n",
        "            return pad_sequence(text_list, padding_value=self.padding_value).to(self.device), torch.tensor(label_list).to(self.device)\n",
        "\n",
        "\n",
        "        train_dp_list = list(self.train_dp)\n",
        "        valid_dp_list = list(self.valid_dp)\n",
        "        test_dp_list = list(self.test_dp)\n",
        "\n",
        "        self.train_loader = DataLoader(train_dp_list,\n",
        "                                batch_sampler=CustomBatchSamplerSimilarLength(dataset = train_dp_list,\n",
        "                                                                        batch_size=self.batch_size,\n",
        "                                                                              tokenizer=self.tokenizer,\n",
        "                                                                              shuffle=True),\n",
        "                                collate_fn=collate_batch)\n",
        "        self.valid_loader = DataLoader(valid_dp_list,\n",
        "                                batch_sampler=CustomBatchSamplerSimilarLength(dataset = valid_dp_list,\n",
        "                                                                        batch_size=self.batch_size,\n",
        "                                                                        tokenizer=self.tokenizer,\n",
        "                                                                        shuffle=False),\n",
        "                                collate_fn=collate_batch)\n",
        "        self.test_loader = DataLoader(test_dp_list,\n",
        "                                batch_sampler=CustomBatchSamplerSimilarLength(dataset = test_dp_list,\n",
        "                                                                        batch_size=self.batch_size,\n",
        "                                                                        tokenizer=self.tokenizer,\n",
        "                                                                        shuffle=False),\n",
        "                                collate_fn=collate_batch)\n",
        "\n",
        "        if debug:\n",
        "            print('Train')\n",
        "            count = 20\n",
        "            for text_batch, label_batch in self.train_loader:\n",
        "                print(f'Text matrix size: {text_batch.size()}')\n",
        "                print(f'Target vector size: {label_batch.size()}')\n",
        "                count -= 1\n",
        "                if count <= 0:\n",
        "                    count = 5\n",
        "                    break\n",
        "\n",
        "            print('\\nValid:')\n",
        "            for text_batch, label_batch in self.valid_loader:\n",
        "                print(f'Text matrix size: {text_batch.size()}')\n",
        "                print(f'Target vector size: {label_batch.size()}')\n",
        "                count -= 1\n",
        "                if count <= 0:\n",
        "                    count = 5\n",
        "                    break\n",
        "\n",
        "            print('\\nTest:')\n",
        "            for text_batch, label_batch in self.test_loader:\n",
        "                print(f'Text matrix size: {text_batch.size()}')\n",
        "                print(f'Target vector size: {label_batch.size()}')\n",
        "                count -= 1\n",
        "                if count <= 0:\n",
        "                    count = 5\n",
        "                    break\n",
        "\n",
        "            text_batch, label_batch = next(iter(self.train_loader))\n",
        "            print(f'Text matrix size: {text_batch.size()}')\n",
        "            print(f'Target vector size: {label_batch.size()}')\n",
        "\n",
        "            text_batch, label_batch = next(iter(self.train_loader))\n",
        "            print(f'Text matrix size: {text_batch.size()}')\n",
        "            print(f'Target vector size: {label_batch.size()}')\n",
        "\n",
        "\n",
        "    def compute_accuracy_loss(self, model, data_loader):\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "\n",
        "            correct_pred, num_examples = 0, 0\n",
        "            for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "                features = features.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                logits = model(features)\n",
        "                loss = torch.nn.functional.cross_entropy(logits, targets)\n",
        "                running_loss += loss.item() * targets.size(0)\n",
        "\n",
        "                _, predicted_labels = torch.max(logits, 1)\n",
        "\n",
        "                num_examples += targets.size(0)\n",
        "                correct_pred += (predicted_labels == targets.float()).sum().to(\"cpu\")\n",
        "\n",
        "        return float(correct_pred)/num_examples * 100, running_loss / num_examples\n",
        "\n",
        "    def build_model(self, model_name: str,\n",
        "                    embedding_dim: int,\n",
        "                    hidden_dim: int,\n",
        "                    num_layers: int,\n",
        "                    num_classes: int,\n",
        "                    debug=False):\n",
        "        if model_name == 'lstm':\n",
        "            self.model = LSTM(input_dim=len(self.vocab),\n",
        "                              embedding_dim=embedding_dim,\n",
        "                              hidden_dim=hidden_dim,\n",
        "                              num_layers=num_layers,\n",
        "                              output_dim=num_classes\n",
        "                              )\n",
        "        elif model_name == 'rnn':\n",
        "            self.model = RNN(input_dim=len(self.vocab),\n",
        "                              embedding_dim=embedding_dim,\n",
        "                              hidden_dim=hidden_dim,\n",
        "                              num_layers=num_layers,\n",
        "                              output_dim=num_classes\n",
        "                              )\n",
        "\n",
        "        elif model_name == 'gru':\n",
        "            self.model = GRU(input_dim=len(self.vocab),\n",
        "                              embedding_dim=embedding_dim,\n",
        "                              hidden_dim=hidden_dim,\n",
        "                              num_layers=num_layers,\n",
        "                              output_dim=num_classes\n",
        "                              )\n",
        "\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        if debug:\n",
        "            print(self.model)\n",
        "\n",
        "    def train_model(self, lr: float,\n",
        "                    num_epochs,\n",
        "                    scheduler_on='valid_acc'):\n",
        "        self.lr = lr\n",
        "        self.num_epochs =num_epochs\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = lr)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer,\n",
        "                                                                    factor=0.1,\n",
        "                                                                    mode='max',\n",
        "                                                                    verbose=True)\n",
        "        self.acc_history = {'train': [], 'valid': []}\n",
        "        self.loss_history = {'train': [], 'valid': []}\n",
        "\n",
        "        start_time = time.time()\n",
        "        minibatch_loss_list = []\n",
        "        # train_acc_list, valid_acc_list = [], []\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "\n",
        "            self.model.train()\n",
        "            for batch_idx, (features, targets) in enumerate(self.train_loader):\n",
        "\n",
        "                features = features.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                # ## FORWARD AND BACK PROP\n",
        "                logits = self.model(features)\n",
        "                loss = torch.nn.functional.cross_entropy(logits, targets)\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # ## UPDATE MODEL PARAMETERS\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # ## LOGGING\n",
        "                minibatch_loss_list.append(loss.item())\n",
        "                # if not batch_idx % logging_interval:\n",
        "                #     print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                #         f'| Batch {batch_idx:04d}/{len(self.train_loader):04d} '\n",
        "                #         f'| Loss: {loss:.4f}')\n",
        "\n",
        "            with torch.no_grad():  # save memory during inference\n",
        "                train_acc, train_loss = self.compute_accuracy_loss(self.model, self.train_loader)\n",
        "                valid_acc, valid_loss = self.compute_accuracy_loss(self.model, self.valid_loader)\n",
        "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                    f'| Train: {train_acc :.2f}% '\n",
        "                    f'| Validation: {valid_acc :.2f}%',\n",
        "                    f'| Train Loss: {train_loss :.4f} '\n",
        "                    f'| Validation Loss: {valid_loss :.4f}',\n",
        "                      )\n",
        "\n",
        "                self.acc_history['train'].append(train_acc)\n",
        "                self.acc_history['valid'].append(valid_acc)\n",
        "                self.loss_history['train'].append(train_loss)\n",
        "                self.loss_history['valid'].append(valid_loss)\n",
        "\n",
        "\n",
        "            elapsed = (time.time() - start_time)/60\n",
        "            print(f'Time elapsed: {elapsed:.2f} min')\n",
        "\n",
        "            if self.scheduler is not None:\n",
        "\n",
        "                if scheduler_on == 'valid_acc':\n",
        "                    self.scheduler.step(self.acc_history['valid'][-1])\n",
        "                elif scheduler_on == 'minibatch_loss':\n",
        "                    self.scheduler.step(minibatch_loss_list[-1])\n",
        "                else:\n",
        "                    raise ValueError(f'Invalid `scheduler_on` choice.')\n",
        "\n",
        "        elapsed = (time.time() - start_time)/60\n",
        "        print(f'Total Training Time: {elapsed:.2f} min')\n",
        "\n",
        "        self.test_acc, self.test_loss = self.compute_accuracy_loss(self.model, self.test_loader)\n",
        "        print(f'Test accuracy {self.test_acc :.2f}%')\n",
        "\n",
        "        # return minibatch_loss_list, train_acc_list, valid_acc_list\n",
        "\n",
        "    def save_results(self, plot=False):\n",
        "\n",
        "        self.exp_name = f'model_{self.model_name}-voacb_{self.vocab_size}-embed_{self.embedding_dim}-hidd_{self.hidden_dim}-num_lyr_{self.num_layers}-bs_{self.batch_size}-lr_{self.lr}-epochs_{self.num_epochs}'\n",
        "\n",
        "        mkdir('stats')\n",
        "        with open(f'stats/{self.exp_name}-stats.pickle', 'wb') as handle:\n",
        "            pickle.dump({\n",
        "                'exp_name': self.exp_name,\n",
        "                'model_name': self.model_name,\n",
        "                'vocab_size': self.vocab_size,\n",
        "                'embedding_dim': self.embedding_dim,\n",
        "                'hidden_dim': self.hidden_dim,\n",
        "                'num_layers': self.num_layers,\n",
        "                'batch_size': self.batch_size,\n",
        "                'num_epochs': self.num_epochs,\n",
        "                'lr': self.lr,\n",
        "                'acc_history': self.acc_history,\n",
        "                'loss_history': self.loss_history,\n",
        "                'test_acc': self.test_acc,\n",
        "                'test_loss': self.test_loss\n",
        "            }, handle)\n",
        "\n",
        "\n",
        "        custom_plot_training_stats(self.acc_history, self.loss_history, ['train', 'valid'], title=self.exp_name, dir='images')"
      ],
      "metadata": {
        "id": "g4i2wGwEmWb5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 123\n",
        "\n",
        "VOCABULARY_SIZE = 40000 # 40000\n",
        "LEARNING_RATE = 0.001 # 0.005\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 5\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "NUM_LAYERS = 3\n",
        "EMBEDDING_DIM = 128 # 128\n",
        "HIDDEN_DIM = 256 # 256\n",
        "NUM_CLASSES = 5\n",
        "\n",
        "\n",
        "rnn_clf = sentiment_clf(DEVICE)\n",
        "db = True\n",
        "rnn_clf.load_data('amazon.csv', debug=db)\n",
        "# rnn_clf.build_vocab(VOCABULARY_SIZE, tokenizer_name='basic_english', debug=db)\n",
        "rnn_clf.build_vocab(VOCABULARY_SIZE, debug=db)\n",
        "rnn_clf.create_dataloaders(BATCH_SIZE, debug=db)\n",
        "rnn_clf.build_model('gru', embedding_dim=EMBEDDING_DIM,\n",
        "                    hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS,\n",
        "                    num_classes=NUM_CLASSES, debug=db)\n",
        "\n",
        "rnn_clf.train_model(\n",
        "    lr=LEARNING_RATE,\n",
        "    num_epochs=NUM_EPOCHS)\n",
        "rnn_clf.save_results()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Sp5XeJKxmYsh",
        "outputId": "dc2dd7b4-9ead-4e78-f34c-e2fc6ccf3b2f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['for computer enthusiast, MaxPC is a welcome sight in your mailbox. i can remember for years savorying every page of \"boot\" (as it was called in beginning) as i was (and still am) obcessed with PC\\'s. Anyone, from advanced users - to beginners looking for knowledge - can profit from every issue of MaxPC. the icing on the cake is the subscription that comes with a CD-ROM as it is packed with demos, utilities, and other useful apps (very helpful for those not blessed with broadband connections). Until I discovered the community of hardware enthusiast web sites, MaxPC, formerly \"boot\", was my only really informative source for computing news and articles. To this day, i consider my subscription to it worth more than 10 subscriptions to most other computing mags. I can\\'t wait until they merge with DVD media and maybe end up offering more info on Divx codecs, encoding your own movies, and best bang for the buck audio and video equipment. Try a few issues (with CD)and you may get hooked...', '4']\n",
            "['Thank god this is not a Ziff Davis publication.  MaxPC will actually tell you if a product is bad. They will print just what they think about something; no sugar coating. I would compare their style to Car and Driver. Technical, but they know how to have a good time.', '4']\n",
            "['Antiques Magazine is a publication made for antique lovers and history buffs and its pages are loaded with photos of artistic paintings, handmade goods, rare pictures, and other items of similar nature. It is a magazine for those who love antiques and also those who consider themselves historians since many articles deal with historical collections of art and antiques.\\n\\nThis magazine is overflowing with antique goods and information. The articles contained in this magazine are intended to be visually stimulating in some cases, educational in others. They offer history lessons on the art that was popular during a specific era, a glimpse into an art collection at a museum, and more. The articles are meant to stimulate the interests of those who already like this subject and they often provide a means to learn more, to discover places to see or purchase some of the featured works, and to develop an appreciation for the craftsmanship that went into making these items.\\n\\nAntiques Magazine is a publication dedicated to creative art and history but one thing that isn\\'t very creative is the titles of the articles. This might surprise some readers at first, but the editors at Antiques decided to get straight to the point in the naming of the featured articles rather than try to be creative. The features thus have titles like \"The Eating Utensil Collection\", \"Spanish Culture in Art\", etc. Each title points out exactly what the article covers, leaving nothing to the imagination.\\n\\nThe writing in Antiques is certainly good and well- edited. But I\\'m afraid that many readers will find the articles a little boring. They describe things well and they usually offer a good history lesson. But they lack spark and readability. I have tried to read some articles in this magazine and found my eyelids getting heavy and my thoughts drifting to other subjects. This is why I generally stick with the pictures when I glance at Antiques Magazine. I don\\'t bother reading because most of the material doesn\\'t keep my interest.\\n\\nOverall, Antiques Magazine is an okay publication and unless you have a deep interest in antiques and their history, you will likely become bored quickly if you try to read the articles. But the pictures in Antiques are often very interesting. Even if you have little interest in reading a magazine like this one, the photos alone make it worth an occasional look.', '2']\n",
            "['This beautiful magazine is in itself a work of art. The quality of every page and bits of information is fascinating. I see the art, the homes and read every word including the ads. I first discovered this gem in London many years ago and now it is in my home and very welcome at that.', '4']\n",
            "['A great read every issue.', '4']\n",
            "[\"This magazine was great for the times but as with all other technology magazines the new stuff isn't as good a lot of advertisments and reviews seem biased.\", '2']\n",
            "[\"I've read Maximum PC (MPC) for many years. The articles in it are not too technical and are written with light humor. That makes it very easy to read while at the same time it is imparting in-depth technical know-how.\\n\\n If you are a newbie, in no time you possess expert knowledge of computers and, to an extent, of electronics.\\n\\nIf you are already advanced, then you get even more advanced. Period.\\n\\nAs an MPC reader you naturally become the PC nerd amongst your friends, family, and coworkers, that everybody keeps coming to for advise and requests for help.\\n\\nFor instance at the launch of new operating system, such as with windows 8, and win 7, you'll get mobbed by confused hordes of PC users with a lost sense of direction.\\n\\nYou become their sole bastion of hope standing solidly there in the middle, that they can hold on to for guidance.\\n\\nThere are also little tid bits of information that can unexpectedly result in huge payoffs. In a little corner in one issue for example, they showed how to insert a check mark in my Utorrent software, that encrypted its outgoing traffic, so that my ISP couldn't invade my privacy anymore, and so had to stop bothering and threatening me about my sharing of copyrighted materials with my friends. That was a relief!\\n\\nI also like the fact that the magazine deals with both computer hardware, and software - though the emphasis is on hardware.\\n\\nThere are also letters to the computer doctor who diagnoses and then gives good advice on a range of computer illnesses and hiccups.\\n\\nYou also get the latest news taking place in the tech world and on copyright issues.\\n\\nOverall It is a well rounded publication with a little for everyone, young and old, great and small.\\n\\nI have the kindle for Ipad version, and it is the absolute best. Better than the Maximum PC app itself that doesn't really work.\", '4']\n",
            "N_ROWS: 89656\n",
            "Num Train: 71724\n",
            "Num Valid: 8966\n",
            "Num Test: 8966\n",
            "Vocabulary size:  40000\n",
            "['<UNK>', '<PAD>', '.', 'the', ',', 'I', 'and', 'to', 'a', ' ']\n",
            "Default index: 0\n",
            "the: 3\n",
            "the: 3\n",
            "<PAD>: 1\n",
            "input to the text_transform: here is an example\n",
            "output of the text_transform: [269, 13, 60, 703]\n",
            "4\n",
            "for computer enthusiast, MaxPC is a welcome sight in your mailbox. i can remember for years savorying every page of \"boot\" (as it was called in beginning) as i was (and still am) obcessed with PC's. Anyone, from advanced users - to beginners looking for knowledge - can profit from every issue of MaxPC. the icing on the cake is the subscription that comes with a CD-ROM as it is packed with demos, utilities, and other useful apps (very helpful for those not blessed with broadband connections). Until I discovered the community of hardware enthusiast web sites, MaxPC, formerly \"boot\", was my only really informative source for computing news and articles. To this day, i consider my subscription to it worth more than 10 subscriptions to most other computing mags. I can't wait until they merge with DVD media and maybe end up offering more info on Divx codecs, encoding your own movies, and best bang for the buck audio and video equipment. Try a few issues (with CD)and you may get hooked...\n",
            "[14, 937, 1642, 4, 11467, 13, 8, 2751, 3577, 15, 81, 861, 2, 154, 64, 655, 14, 68, 0, 98, 217, 10, 26, 5525, 26, 57, 33, 12, 31, 527, 15, 897, 55, 33, 154, 31, 57, 6, 130, 90, 55, 0, 28, 637, 25, 2, 2344, 4, 56, 1295, 2362, 22, 7, 1985, 211, 14, 853, 22, 64, 3499, 56, 98, 43, 10, 11467, 2, 3, 12818, 20, 3, 3673, 13, 3, 37, 17, 410, 28, 8, 1549, 22, 12426, 33, 12, 13, 1074, 28, 3598, 4, 13191, 4, 6, 97, 331, 2535, 57, 67, 327, 14, 195, 27, 7165, 28, 13792, 5388, 55, 2, 3480, 5, 1509, 3, 1534, 10, 1940, 1642, 832, 1992, 4, 11467, 4, 6366, 26, 5525, 26, 4, 31, 19, 80, 87, 220, 583, 14, 7728, 227, 6, 34, 2, 651, 16, 285, 4, 154, 877, 19, 37, 7, 12, 197, 53, 88, 337, 358, 7, 135, 97, 7728, 643, 2, 5, 202, 30, 428, 425, 42, 11888, 28, 2311, 855, 6, 652, 394, 70, 1444, 53, 320, 20, 35222, 0, 4, 21051, 81, 323, 688, 4, 6, 129, 5752, 14, 3, 3347, 2217, 6, 1344, 888, 2, 2131, 8, 169, 92, 57, 28, 33893, 23, 324, 61, 1876, 103]\n",
            "4\n",
            "['for' 'computer' 'enthusiast' ',' 'MaxPC' 'is' 'a' 'welcome' 'sight' 'in'\n",
            " 'your' 'mailbox' '.' 'i' 'can' 'remember' 'for' 'years' '<UNK>' 'every'\n",
            " 'page' 'of' '\"' 'boot' '\"' '(' 'as' 'it' 'was' 'called' 'in' 'beginning'\n",
            " ')' 'as' 'i' 'was' '(' 'and' 'still' 'am' ')' '<UNK>' 'with' 'PC' \"'s\"\n",
            " '.' 'Anyone' ',' 'from' 'advanced' 'users' '-' 'to' 'beginners' 'looking'\n",
            " 'for' 'knowledge' '-' 'can' 'profit' 'from' 'every' 'issue' 'of' 'MaxPC'\n",
            " '.' 'the' 'icing' 'on' 'the' 'cake' 'is' 'the' 'subscription' 'that'\n",
            " 'comes' 'with' 'a' 'CD' '-' 'ROM' 'as' 'it' 'is' 'packed' 'with' 'demos'\n",
            " ',' 'utilities' ',' 'and' 'other' 'useful' 'apps' '(' 'very' 'helpful'\n",
            " 'for' 'those' 'not' 'blessed' 'with' 'broadband' 'connections' ')' '.'\n",
            " 'Until' 'I' 'discovered' 'the' 'community' 'of' 'hardware' 'enthusiast'\n",
            " 'web' 'sites' ',' 'MaxPC' ',' 'formerly' '\"' 'boot' '\"' ',' 'was' 'my'\n",
            " 'only' 'really' 'informative' 'source' 'for' 'computing' 'news' 'and'\n",
            " 'articles' '.' 'To' 'this' 'day' ',' 'i' 'consider' 'my' 'subscription'\n",
            " 'to' 'it' 'worth' 'more' 'than' '10' 'subscriptions' 'to' 'most' 'other'\n",
            " 'computing' 'mags' '.' 'I' 'ca' \"n't\" 'wait' 'until' 'they' 'merge'\n",
            " 'with' 'DVD' 'media' 'and' 'maybe' 'end' 'up' 'offering' 'more' 'info'\n",
            " 'on' 'Divx' '<UNK>' ',' 'encoding' 'your' 'own' 'movies' ',' 'and' 'best'\n",
            " 'bang' 'for' 'the' 'buck' 'audio' 'and' 'video' 'equipment' '.' 'Try' 'a'\n",
            " 'few' 'issues' '(' 'with' 'CD)and' 'you' 'may' 'get' 'hooked' '...']\n",
            "Train\n",
            "Text matrix size: torch.Size([10, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([10, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([15, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([37, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([33, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([130, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([153, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([3, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([18, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([42, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([98, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([79, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([33, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([14, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([48, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([31, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([28, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([40, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([6, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([53, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([1, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([1, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([2, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([2, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([3, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([1, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([2, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([2, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([2, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([3, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([4, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "Text matrix size: torch.Size([26, 64])\n",
            "Target vector size: torch.Size([64])\n",
            "GRU(\n",
            "  (embedding): Embedding(40000, 128)\n",
            "  (gru): GRU(128, 256, num_layers=3)\n",
            "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Epoch: 001/005 | Train: 70.45% | Validation: 68.76% | Train Loss: 0.7918 | Validation Loss: 0.8271\n",
            "Time elapsed: 0.88 min\n",
            "Epoch: 002/005 | Train: 73.39% | Validation: 70.66% | Train Loss: 0.6951 | Validation Loss: 0.7769\n",
            "Time elapsed: 1.75 min\n",
            "Epoch: 003/005 | Train: 78.48% | Validation: 71.57% | Train Loss: 0.5830 | Validation Loss: 0.7531\n",
            "Time elapsed: 2.64 min\n",
            "Epoch: 004/005 | Train: 82.55% | Validation: 71.29% | Train Loss: 0.4694 | Validation Loss: 0.7738\n",
            "Time elapsed: 3.49 min\n",
            "Epoch: 005/005 | Train: 86.45% | Validation: 71.10% | Train Loss: 0.3771 | Validation Loss: 0.8586\n",
            "Time elapsed: 4.36 min\n",
            "Total Training Time: 4.36 min\n",
            "Test accuracy 70.95%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBxX53aOm3xp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfe95fa9-3a7e-4270-a623-dfa5a44cc93c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3dN--C4Pm0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for _, _, files in os.walk(\"stats\"):\n",
        "    temp = files"
      ],
      "metadata": {
        "id": "GPBXwVDt_cNF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kmmZVZuuPnh9"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_res = {'model_name': [], 'test_acc': []}\n",
        "\n",
        "for pkl_file in temp:\n",
        "    with open(f'stats/{pkl_file}', \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "        model_name = data['model_name']\n",
        "        num_layer = data['num_layers']\n",
        "        dict_res['model_name'].append(f'{model_name}<{num_layer}>')\n",
        "        dict_res['test_acc'].append(data['test_acc'])\n",
        "\n",
        "        # print(data['test_acc'], data['exp_name'])"
      ],
      "metadata": {
        "id": "AUwvCDXSNpkS"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install tabulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4BEW5JjOUM_",
        "outputId": "4073fc9a-ceed-4565-ccdb-8a2f0e7b23f7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.8.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "rtOIzC5tP66P"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tabulate(dict_res, headers=\"keys\", tablefmt=\"latex\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3lrEmBLQZU6",
        "outputId": "ee39c7f4-8fc2-4c60-dab1-4c2234369a5e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lr}\n",
            "\\hline\n",
            " model\\_name   &   test\\_acc \\\\\n",
            "\\hline\n",
            " lstm\\ensuremath{<}2\\ensuremath{>}      &    70.6447 \\\\\n",
            " gru\\ensuremath{<}2\\ensuremath{>}       &    71.1912 \\\\\n",
            " rnn\\ensuremath{<}2\\ensuremath{>}       &    61.3875 \\\\\n",
            " rnn\\ensuremath{<}1\\ensuremath{>}       &    59.8706 \\\\\n",
            " gru\\ensuremath{<}1\\ensuremath{>}       &    71.3362 \\\\\n",
            " lstm\\ensuremath{<}1\\ensuremath{>}      &    70.6893 \\\\\n",
            " lstm\\ensuremath{<}3\\ensuremath{>}      &    69.7078 \\\\\n",
            " rnn\\ensuremath{<}3\\ensuremath{>}       &    11.6551 \\\\\n",
            " rnn\\ensuremath{<}3\\ensuremath{>}       &    65.0457 \\\\\n",
            " gru\\ensuremath{<}3\\ensuremath{>}       &    70.9458 \\\\\n",
            "\\hline\n",
            "\\end{tabular}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Va0k5o5WQkUp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}